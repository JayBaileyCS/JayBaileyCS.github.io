---
layout: archive
title: "Writing"
permalink: /writing/
author_profile: true
---

## Writing

This page is for non-technical writing that I've done that doesn't fall under the purview of research.

### Articles

[Deep Q-Networks Explained](https://www.lesswrong.com/posts/kyvCNgx9oAwJCuevo/deep-q-networks-explained) is an article about the first major RL algorithm that DeepMind used to train RL agents to play Atari games. This article was used in ARENA Virtual as material for the Reinforcement Learning section.

[Reflections On My 5-month AI Alignment Upskilling Grant](https://forum.effectivealtruism.org/posts/DnMg5q4Wyuuf99kkX/reflections-on-my-5-month-ai-alignment-upskilling-grant) is an article about my first grant to upskill in maths and ML, before I attended SERI MATS. It proved useful to quite a few people and I still occasionally get people reaching out to ask questions about it. (You're' welcome to do this too, if you are one of those curious people!)

### Tutorials

[TransformerLens: Head Detector Demo](https://colab.research.google.com/github/neelnanda-io/TransformerLens/blob/main/demos/Head_Detector_Demo.ipynb). A [TransformerLens](https://github.com/neelnanda-io/TransformerLens) feature that allows users to automatically check how similar an attention head is to several simple token heads (current token, previous token, induction head) as well as create their own metrics.

[Transformer Lens: SVD Interpreter Demo](https://colab.research.google.com/github/neelnanda-io/TransformerLens/blob/main/demos/SVD_Interpreter_Demo.ipynb). After research at Conjecture showed the SVD's of Transformers led to [semantically interpretable clusters](https://www.lesswrong.com/posts/mkbGjzxD8d8XqKHzA/the-singular-value-decompositions-of-transformer-weight#Directly_editing_SVD_representations), I wrote up a feature to perform this decomposition in TransformerLens.
